{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Deep Q Network </center>#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Deep Q Netowrk to play the CartPole game: balance the pole on the cart. \n",
    "\n",
    "CartPole gym environment is made up the following:\n",
    "\n",
    "**Observation (State)**:\n",
    "\n",
    "\n",
    "|Num\t|Observation\t|Min\t|Max|\n",
    "|:------:|:------------|---------:|-----:|\n",
    "|0\t|Cart Position\t|-2.4|\t2.4|\n",
    "|1\t|Cart Velocity|\t-Inf\t|Inf|\n",
    "|2\t|Pole Angle\t|~ -41.8°|\t~ 41.8°|\n",
    "|3\t|Pole Velocity At Tip\t|-Inf\t|Inf|\n",
    "\n",
    "**Actions**\n",
    "\n",
    "|Num\t|Action   |\n",
    "|:------:|:------------|\n",
    "|0\tPush |cart to the left|\n",
    "|1\tPush |cart to the right|\n",
    "\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "**Reward**\n",
    "\n",
    "Reward is 1 for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a state as an input, we use a sequence neural network to predict the Q-value for each possible state (state-action values)\n",
    "\n",
    "For Deep Q Network, the loss is the difference between the predicted Q value and the expected Q value. Then `MSE` is the loss function used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_model(self):\n",
    "    \n",
    "    # Neural Net for Deep-Q learning Model\n",
    "    model = Sequential()\n",
    "    # input is a state, so the input dimension is the state shape\n",
    "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))  \n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    # The output dimension is the number of actions\n",
    "    # Q values can be positive or negative. So activation function is linear\n",
    "    model.add(Dense(self.action_size, activation='linear'))\n",
    "    model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the deep neural network by experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym environments are structured around two main parts: an observation space (i.e. state) and an action space. We observe the former from the environment and use that to determine how best to update it through an action, i.e. based on the current state of the pole (observation), determine whether to move the cart left or right (action).\n",
    "\n",
    "The first part of any machine learning problem is gathering the data, and this one is no different. Luckily, OpenAI’s gym environment provides a very straightforward way of gathering data: we can essentially just run through the simulation many times and take random steps every time. \n",
    "\n",
    "The training samples, denoted as tuples of $(s, a, r, s', done)$, are collected through the `memorize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experience as samples\n",
    "# done : whether end game is reached\n",
    "def memorize(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we `replay` the experiences $(s, a, r, s')$ collected:\n",
    "- Predict Q value of $Q_{predicted}(s, a)$ using the deep neural network\n",
    "- Calculate the target Q value of of $(s, a)$ as: $Q_{target}(s, a) = r + \\gamma * \\max_{a'}{Q(s',a')}$, where $Q(s',a')$ is predicted with the deep learning network. \n",
    "- Finally, the loss is measured as $ MSE~[Q_{predicted}(s, a), Q_{target}(s, a)]$, the difference between the `predicted Q value` and `target Q value`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiences and train the deep learning model following mini-batch\n",
    "def replay(self, batch_size):\n",
    "    \n",
    "    # sample a minibatch\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "    \n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = (reward + self.gamma *\n",
    "                      np.amax(self.model.predict(next_state)[0]))\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        # apply smaller epsilon to reduce exploration\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep neural network predicts the Q value for all actions given an input state. Then the action with the max Q value is selected. Meanwhile, $\\epsilon$-greedy is implemented to choose a random action for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(self, state):\n",
    "    # e-greedy \n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        return random.randrange(self.action_size)\n",
    "    \n",
    "    act_values = self.model.predict(state)\n",
    "    # returns action with the max Q value\n",
    "    return np.argmax(act_values[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # initialization for agent and environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "\n",
    "    # hyper-parameter\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    history = []\n",
    "    EPISODES = 100\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        \n",
    "        # initialize state\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size]) # an array containing only one array [[a,b,c,d]]\n",
    "        rewards = 0\n",
    "        \n",
    "        for time in range(500): # need to be modified for time range within a year\n",
    "            # env.render()\n",
    "            \n",
    "            # take an action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # environment responds to the action and return new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # log reward. If done (i.e. the pole falls down, the reward is -10)\n",
    "            reward = reward if not done else -10\n",
    "            rewards += reward\n",
    "            \n",
    "            # reshape state since the neural network expects an array with rank 2\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # save the experience for replay\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            \n",
    "            # save the new state as the current state and go for next round\n",
    "            state = next_state\n",
    "            \n",
    "            # determine if the game is over or not\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # if there are enough experiences accumulated, replay to train the network    \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "                \n",
    "        print(\"episode: {}/{}, score: {}, e: {:.2}, rewards: {}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon, rewards))\n",
    "        \n",
    "        history.append([e, time, agent.epsilon, rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history, columns =[\"episode\", \"total_time\",\"epsilon\",'reward'])\n",
    "df.set_index(\"episode\")\n",
    "df['reward'].rolling(5).mean().plot(figsize=(15,5))\n",
    "plt.show\n",
    "\n",
    "df[[\"epsilon\"]].plot(figsize=(15,5))\n",
    "plt.show"
   ]
  },
  {
   "source": [
    "# Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Integer: 12, Float number: 345.68\n"
     ]
    }
   ],
   "source": [
    "print('Integer: %d, Float number: %.2f' % (12, 345.678))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([ 0.03122584, -0.00282333, -0.03471958, -0.02291136]), 1.0, False, {})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "source": [
    "ENV:\n",
    "- self.observation_space (np.array) {need modification for both agent and env}\n",
    "- self.action_space (np.array) {need modification for both agent and env}\n",
    "- step {return a four elements tuple (next_state, reward, done, )}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modify the code to play a more complex game: Mountain car\n",
    "- Quite often, for better convergence, you may need two networks, one for predicting actions, one for doing predictions and one for tracking “target values”, for better neural network convergency. See this article for the details: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mountain Car Environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
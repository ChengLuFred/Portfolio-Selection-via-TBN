{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients - REINFORCE\n",
    "\n",
    "References:\n",
    "- https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff\n",
    "- https://github.com/keon/policy-gradient/blob/master/pg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the REINFORCE policy gradient to play game \"Pong\" using OpenAI gym environment for <a href= https://gym.openai.com/envs/Pong-v0/>Pong </a>â€” a self-contained instance of the game that facilitates interfacing with the permissible actions within the game. To install the game, issue the following: \n",
    "\n",
    "`pip install gym[atari]`\n",
    "\n",
    "Different from other experiments we have done so far, the states in this game is an image. We'll use convolutional neural networks to extract features from the state space and predict a good action. Although here we have discrete actions, certainly, you can also use REINFORCE algorithm for continuous actions, e.g. trading stocks. \n",
    "\n",
    "Now let's get start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, Flatten, Input, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9661ea65ae88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Play the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pong-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/atari/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     raise error.DependencyNotInstalled(\n\u001b[1;32m     12\u001b[0m             \u001b[0;34m\"{}. (HINT: you can install Atari dependencies by running \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \"'pip install gym[atari]'.)\".format(e))\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "# Play the game \n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = np.random.choice(env.action_space.n)\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()    # close the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six actions an agent (player) can take within the Pong environment, each assigned an integer, e.g.:\n",
    "\n",
    "- remaining stationary (0), \n",
    "- vertical translation up (2), \n",
    "- vertical translation down (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from the previous examples, the states in this environment is an image, a three-dimensional array. However, note that this frame contains two many unnecessary details irrelevant to the actual gameplay itself: for example, the tracking of the rewards is done internally by the OpenAI gym environment. Weâ€™ll preprocess our frame by applying a crop and a grayscale, before flattening the output for a 1-dimensional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to remove unnecessary details\n",
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error \n",
    "# install Atari by issuing: pip install gym[atari] \n",
    "\n",
    "state = env.reset()\n",
    "#env.render()\n",
    "print(state.shape)\n",
    "\n",
    "# show original state\n",
    "plt.imshow(state)\n",
    "plt.show()\n",
    "\n",
    "# show preprocessed state\n",
    "plt.imshow(preprocess(state))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With states identified, we start to define our action network. Naturally, we'll use convolutional neural network to process the input state and return the probability of each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_model(self):\n",
    "\n",
    "    states = Input(shape = self.state_size + (1,) )  # input shape is (80, 80, 1)\n",
    "    rewards = Input(shape = (1,))   # accumulated returns at each time, for calculating gradients\n",
    "\n",
    "    conv = Conv2D(16, (3,3), padding='same', activation='relu')(states)\n",
    "    pool = MaxPool2D((2,2))(conv)\n",
    "    conv = Conv2D(64, (3,3), padding='same', activation='relu')(pool)\n",
    "    pool = MaxPool2D((2,2))(conv)\n",
    "    conv = Conv2D(32, (3,3), padding='same', activation='relu')(pool)\n",
    "    flat = Flatten()(conv)\n",
    "    h = Dense(256, activation='relu', init='he_uniform')(flat)\n",
    "    output = Dense(self.action_size, activation='softmax') (h)\n",
    "\n",
    "    model = Model(inputs = [states, rewards], outputs = output)\n",
    "\n",
    "    opt = Adam(lr=self.learning_rate)\n",
    "\n",
    "    policy_loss = self.policy_gradient_loss(rewards)\n",
    "\n",
    "    model.compile(loss=policy_loss, optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the gradients for parameters $\\theta$ in the action neural network at time $t$ is :\n",
    "\n",
    "$$ \\theta \\rightarrow \\theta + \\alpha~\\gamma^t~G~\\nabla \\ln \\pi(A_t | S_t, \\theta))$$       \n",
    "\n",
    "Let's first calculate $$G = \\sum_{k=t+1}^T {\\gamma^{k-t-1}R_k}$$\n",
    "\n",
    "Let $G_t$ denote the discounted reward received at time $t$. Suppose we have played an episode from ${0, 1, ..., T-1}$, we accumulate the rewards received at each step backwards, i.e.:\n",
    "<br><br>\n",
    "$$G_{T-1} = R_{T-1}$$ \n",
    "$$G_{T-2} = R_{T-2} + \\gamma * G_{T-1} $$ \n",
    "...<br>\n",
    "$$G_{0} = R_0 +\\gamma * G_{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(self, rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        if rewards[t] != 0:   # For pong, a non-zero reward indicating the end of an episode, so the running reward is reset to 0\n",
    "            running_add = 0   \n",
    "        running_add = running_add * self.gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network with parameters $\\theta$, let's assume the following: \n",
    "1. it has prediction $\\hat{y}$, \n",
    "2. the actual value is $y$, and \n",
    "3. the loss function is `categorical_cross_entropy`, \n",
    "\n",
    "Then the gradients can be written as $L = y~\\nabla_{\\theta}\\ln\\hat{y}$. If we multiply this loss by the `discounted reward` $\\gamma^t~G$, i.e.  $L' = L * \\gamma^t~G$, then we'll have the exact gradient formula used in REINFORCE algorithm. Moreover, we normalize $\\gamma^t~G$ so that the training can converge fast.\n",
    "\n",
    "Let's make our neural network to have an additional input for `discounted reward`. Then we create a custom loss function. Note that a custom loss function always only takes two inputs: ground truth $y$ and predicted $\\hat{y}$. However, we need to pass the additional parameter, `discounted reward`. To achieve this, we play a trick to define a function which has the `discounted reward` as the input parameter, and returns the a loss function with ground truth $y$ and predicted $\\hat{y}$. The `discounted reward` as a global variable now can be passed to the loss function. The code can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss(self, Returns):\n",
    "    def modified_crossentropy(action,action_probs):\n",
    "        cost = K.categorical_crossentropy(action, action_probs,\\\n",
    "                                          from_logits=False, axis=1)\n",
    "        g = K.squeeze(Returns, axis = -1)\n",
    "        cost = cost * g\n",
    "        return K.mean(cost)\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is trained as usual. Note that there are two inputs: One is the stack of history states in an episode, and the other is the discounted rewards. The output is the actual actions taken in the episode. The actions are sampled by the probabilities output by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    rewards = np.vstack(self.rewards)\n",
    "    rewards = self.discount_rewards(rewards)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n",
    "\n",
    "    # reshape states to [Batch, 80, 80, 1]\n",
    "    X = np.stack(self.states, axis = 0)\n",
    "    X = np.expand_dims(X, axis = -1) \n",
    "\n",
    "    Y = np.vstack(self.actions)\n",
    "\n",
    "    self.model.train_on_batch([X, rewards], Y)\n",
    "\n",
    "    self.states, self.probs, self.actions, self.rewards = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game very well after training for 500 episodes. You can also load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        #self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.model = self._build_model()\n",
    "        self.model.summary()\n",
    "        \n",
    "        \n",
    "    def policy_gradient_loss(self, Returns):\n",
    "        def modified_crossentropy(action,action_probs):\n",
    "            cost = K.categorical_crossentropy(action, action_probs,\\\n",
    "                                              from_logits=False, axis=1)\n",
    "            g = K.squeeze(Returns, axis = -1)\n",
    "            cost = cost * g\n",
    "            return K.mean(cost)\n",
    "        return modified_crossentropy\n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "        states = Input(shape = self.state_size + (1,) )  # input shape is (80, 80, 1)\n",
    "        rewards = Input(shape = (1,))   # accumulated returns at each time\n",
    "        \n",
    "        conv = Conv2D(16, (3,3), padding='same', activation='relu')(states)\n",
    "        pool = MaxPool2D((2,2))(conv)\n",
    "        conv = Conv2D(64, (3,3), padding='same', activation='relu')(pool)\n",
    "        pool = MaxPool2D((2,2))(conv)\n",
    "        conv = Conv2D(32, (3,3), padding='same', activation='relu')(pool)\n",
    "        flat = Flatten()(conv)\n",
    "        h = Dense(256, activation='relu', init='he_uniform')(flat)\n",
    "        output = Dense(self.action_size, activation='softmax') (h)\n",
    "        \n",
    "        model = Model(inputs = [states, rewards], outputs = output)\n",
    "        \n",
    "        opt = Adam(lr=self.learning_rate)\n",
    "        \n",
    "        policy_loss = self.policy_gradient_loss(rewards)\n",
    "        \n",
    "        model.compile(loss=policy_loss, optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, prob, reward):\n",
    "        y = np.zeros([self.action_size])\n",
    "        y[action] = 1\n",
    "        #self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        self.actions.append(y)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.expand_dims(state, axis = -1) # rehape to (80, 80, 1)\n",
    "        state = np.expand_dims(state, axis = 0) # rehape to (1, 80, 80, 1)\n",
    "        prob = self.model.predict([state, np.zeros((1,1))]).flatten()\n",
    "        self.probs.append(prob)\n",
    "        #prob = aprob / np.sum(aprob)\n",
    "        action = np.random.choice(self.action_size, 1, p=prob)[0]\n",
    "        return action, prob\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, rewards.size)):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        rewards = np.vstack(self.rewards)\n",
    "        rewards = self.discount_rewards(rewards)\n",
    "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n",
    "        \n",
    "        # reshape states to [Batch, 80, 80, 1]\n",
    "        X = np.stack(self.states, axis = 0)\n",
    "        X = np.expand_dims(X, axis = -1) \n",
    "        \n",
    "        Y = np.vstack(self.actions)\n",
    "        \n",
    "        self.model.train_on_batch([X, rewards], Y)\n",
    "        \n",
    "        self.states, self.probs, self.actions, self.rewards = [], [], [], []\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 80, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 40, 64)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 3,306,502\n",
      "Trainable params: 3,306,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Episode: 1 - Score: -13.00 - steps 3910 - time: 7.81\n",
      "Episode: 2 - Score: -4.00 - steps 5756 - time: 9.93\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "    total_episodes = 10000\n",
    "\n",
    "    state_size = (80, 80)\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size)\n",
    "    steps = 0\n",
    "    agent.load('pong.h5')\n",
    "    while episode < total_episodes:\n",
    "        env.render()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        cur_x = preprocess(state)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(state_size)\n",
    "        \n",
    "        prev_x = cur_x\n",
    "        #print(cur_x.shape)\n",
    "\n",
    "        action, prob = agent.act(x)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        #print(reward)\n",
    "        score += reward\n",
    "        \n",
    "        agent.memorize(x, action, prob, reward)\n",
    "        steps += 1\n",
    "        if done:\n",
    "            \n",
    "            episode += 1\n",
    "            agent.train()\n",
    "            print('Episode: %d - Score: %.2f - steps %d - time: %.2f' % (episode, score, steps, time.time()-start_time))\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            prev_x = None\n",
    "            steps = 0\n",
    "            if episode > 1 and episode % 10 == 0:\n",
    "                agent.save('pong.h5')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step (Optional Assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the code to implement algorithms Actor-Critic and A2C (Advantage Actor-Critic). A good reference can be found at https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "- Compare the performance of these three algorithms to which one converges better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients - REINFORCE\n",
    "\n",
    "References:\n",
    "- https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff\n",
    "- https://github.com/keon/policy-gradient/blob/master/pg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the REINFORCE policy gradient to play game \"Pong\" using OpenAI gym environment for <a href= https://gym.openai.com/envs/Pong-v0/>Pong </a>— a self-contained instance of the game that facilitates interfacing with the permissible actions within the game. To install the game, issue the following: \n",
    "\n",
    "`pip install gym[atari]`\n",
    "\n",
    "Different from other experiments we have done so far, the states in this game is an image. We'll use convolutional neural networks to extract features from the state space and predict a good action. Although here we have discrete actions, certainly, you can also use REINFORCE algorithm for continuous actions, e.g. trading stocks. \n",
    "\n",
    "Now let's get start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, Flatten, Input, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the game \n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "#env = wrappers.Monitor(env, video_callable=False ,force=True)\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    #env.render()\n",
    "    action = np.random.choice(env.action_space.n)\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()    # close the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six actions an agent (player) can take within the Pong environment, each assigned an integer, e.g.:\n",
    "\n",
    "- remaining stationary (0), \n",
    "- vertical translation up (2), \n",
    "- vertical translation down (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from the previous examples, the states in this environment is an image, a three-dimensional array. However, note that this frame contains two many unnecessary details irrelevant to the actual gameplay itself: for example, the tracking of the rewards is done internally by the OpenAI gym environment. We’ll preprocess our frame by applying a crop and a grayscale, before flattening the output for a 1-dimensional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to remove unnecessary details\n",
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if you get an error \n",
    "# install Atari by issuing: pip install gym[atari] \n",
    "\n",
    "state = env.reset()\n",
    "#env.render()\n",
    "print(state.shape)\n",
    "\n",
    "# show original state\n",
    "plt.imshow(state)\n",
    "plt.show()\n",
    "\n",
    "# show preprocessed state\n",
    "plt.imshow(preprocess(state))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(state).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With states identified, we start to define our action network. Naturally, we'll use convolutional neural network to process the input state and return the probability of each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_model(self):\n",
    "\n",
    "    states = Input(shape = self.state_size + (1,) )  # input shape is (80, 80, 1)\n",
    "    rewards = Input(shape = (1,))   # accumulated returns at each time, for calculating gradients\n",
    "\n",
    "    conv = Conv2D(16, (3,3), padding='same', activation='relu')(states)\n",
    "    pool = MaxPool2D((2,2))(conv)\n",
    "    conv = Conv2D(64, (3,3), padding='same', activation='relu')(pool)\n",
    "    pool = MaxPool2D((2,2))(conv)\n",
    "    conv = Conv2D(32, (3,3), padding='same', activation='relu')(pool)\n",
    "    flat = Flatten()(conv)\n",
    "    h = Dense(256, activation='relu', init='he_uniform')(flat)\n",
    "    output = Dense(self.action_size, activation='softmax') (h)\n",
    "\n",
    "    model = Model(inputs = [states, rewards], outputs = output)\n",
    "\n",
    "    opt = Adam(lr=self.learning_rate)\n",
    "\n",
    "    policy_loss = self.policy_gradient_loss(rewards)\n",
    "\n",
    "    model.compile(loss=policy_loss, optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the gradients for parameters $\\theta$ in the action neural network at time $t$ is :\n",
    "\n",
    "$$ \\theta \\rightarrow \\theta + \\alpha~\\gamma^t~G~\\nabla \\ln \\pi(A_t | S_t, \\theta))$$       \n",
    "\n",
    "Let's first calculate $$G = \\sum_{k=t+1}^T {\\gamma^{k-t-1}R_k}$$\n",
    "\n",
    "Let $G_t$ denote the discounted reward received at time $t$. Suppose we have played an episode from ${0, 1, ..., T-1}$, we accumulate the rewards received at each step backwards, i.e.:\n",
    "<br><br>\n",
    "$$G_{T-1} = R_{T-1}$$ \n",
    "$$G_{T-2} = R_{T-2} + \\gamma * G_{T-1} $$ \n",
    "...<br>\n",
    "$$G_{0} = R_0 +\\gamma * G_{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(self, rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        if rewards[t] != 0:   # For pong, a non-zero reward indicating the end of an episode, so the running reward is reset to 0\n",
    "            running_add = 0   \n",
    "        running_add = running_add * self.gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network with parameters $\\theta$, let's assume the following: \n",
    "1. it has prediction $\\hat{y}$, \n",
    "2. the actual value is $y$, and \n",
    "3. the loss function is `categorical_cross_entropy`, \n",
    "\n",
    "Then the gradients can be written as $L = y~\\nabla_{\\theta}\\ln\\hat{y}$. If we multiply this loss by the `discounted reward` $\\gamma^t~G$, i.e.  $L' = L * \\gamma^t~G$, then we'll have the exact gradient formula used in REINFORCE algorithm. Moreover, we normalize $\\gamma^t~G$ so that the training can converge fast.\n",
    "\n",
    "Let's make our neural network to have an additional input for `discounted reward`. Then we create a custom loss function. Note that a custom loss function always only takes two inputs: ground truth $y$ and predicted $\\hat{y}$. However, we need to pass the additional parameter, `discounted reward`. To achieve this, we play a trick to define a function which has the `discounted reward` as the input parameter, and returns the a loss function with ground truth $y$ and predicted $\\hat{y}$. The `discounted reward` as a global variable now can be passed to the loss function. The code can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss(self, Returns):\n",
    "    def modified_crossentropy(action,action_probs):\n",
    "        cost = K.categorical_crossentropy(action, action_probs,\\\n",
    "                                          from_logits=False, axis=1)\n",
    "        g = K.squeeze(Returns, axis = -1)\n",
    "        cost = cost * g\n",
    "        return K.mean(cost)\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is trained as usual. Note that there are two inputs: One is the stack of history states in an episode, and the other is the discounted rewards. The output is the actual actions taken in the episode. The actions are sampled by the probabilities output by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    rewards = np.vstack(self.rewards)\n",
    "    rewards = self.discount_rewards(rewards)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7) # normalization\n",
    "\n",
    "    # reshape states to [Batch, 80, 80, 1]\n",
    "    X = np.stack(self.states, axis = 0)\n",
    "    X = np.expand_dims(X, axis = -1) \n",
    "\n",
    "    Y = np.vstack(self.actions)\n",
    "\n",
    "    self.model.train_on_batch([X, rewards], Y)\n",
    "\n",
    "    self.states, self.probs, self.actions, self.rewards = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_test = np.vstack(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(state_test, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game very well after training for 500 episodes. You can also load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        #self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.model = self._build_model()\n",
    "        self.model.summary()\n",
    "        \n",
    "        \n",
    "    def policy_gradient_loss(self, Returns):\n",
    "        def modified_crossentropy(action,action_probs):\n",
    "            cost = K.categorical_crossentropy(action, action_probs,\\\n",
    "                                              from_logits=False, axis=1)\n",
    "            g = K.squeeze(Returns, axis = -1)\n",
    "            cost = cost * g\n",
    "            return K.mean(cost)\n",
    "        return modified_crossentropy\n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "        states = Input(shape = self.state_size + (1,) )  # input shape is (80, 80, 1)\n",
    "        rewards = Input(shape = (1,))   # accumulated returns at each time\n",
    "        \n",
    "        conv = Conv2D(16, (3,3), padding='same', activation='relu')(states)\n",
    "        pool = MaxPool2D((2,2))(conv)\n",
    "        conv = Conv2D(64, (3,3), padding='same', activation='relu')(pool)\n",
    "        pool = MaxPool2D((2,2))(conv)\n",
    "        conv = Conv2D(32, (3,3), padding='same', activation='relu')(pool)\n",
    "        flat = Flatten()(conv)\n",
    "        # h = Dense(256, activation='relu', init='he_uniform')(flat)\n",
    "        h = Dense(256, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform())(flat)\n",
    "        output = Dense(self.action_size, activation='softmax') (h)\n",
    "        \n",
    "        model = Model(inputs = [states, rewards], outputs = output)\n",
    "        \n",
    "        opt = Adam(lr=self.learning_rate)\n",
    "        \n",
    "        policy_loss = self.policy_gradient_loss(rewards)\n",
    "        \n",
    "        model.compile(loss=policy_loss, optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, prob, reward):\n",
    "        y = np.zeros([self.action_size])\n",
    "        y[action] = 1\n",
    "        #self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        self.actions.append(y)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.expand_dims(state, axis = -1) # rehape to (80, 80, 1)\n",
    "        state = np.expand_dims(state, axis = 0) # rehape to (1, 80, 80, 1)\n",
    "        prob = self.model.predict([state, np.zeros((1,1))]).flatten()\n",
    "        self.probs.append(prob)\n",
    "        #prob = aprob / np.sum(aprob)\n",
    "        action = np.random.choice(self.action_size, 1, p=prob)[0]\n",
    "        return action, prob\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, rewards.size)):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        rewards = np.vstack(self.rewards)\n",
    "        rewards = self.discount_rewards(rewards)\n",
    "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n",
    "        \n",
    "        # reshape states to [Batch, 80, 80, 1]\n",
    "        X = np.stack(self.states, axis = 0)\n",
    "        X = np.expand_dims(X, axis = -1) \n",
    "        \n",
    "        Y = np.vstack(self.actions)\n",
    "        \n",
    "        self.model.train_on_batch([X, rewards], Y)\n",
    "        \n",
    "        self.states, self.probs, self.actions, self.rewards = [], [], [], []\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80, 80, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 80, 80, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 40, 40, 16)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 40, 40, 64)   9280        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 20, 20, 32)   18464       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          3277056     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,306,502\n",
      "Trainable params: 3,306,502\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 - Score: -20.00 - steps 1243 - time: 2.12\n",
      "Episode: 2 - Score: -21.00 - steps 1214 - time: 1.98\n",
      "Episode: 3 - Score: -20.00 - steps 1351 - time: 2.22\n",
      "Episode: 4 - Score: -19.00 - steps 1355 - time: 2.43\n",
      "Episode: 5 - Score: -19.00 - steps 1431 - time: 2.45\n",
      "Episode: 6 - Score: -21.00 - steps 1192 - time: 2.06\n",
      "Episode: 7 - Score: -20.00 - steps 1117 - time: 1.94\n",
      "Episode: 8 - Score: -20.00 - steps 1224 - time: 2.29\n",
      "Episode: 9 - Score: -20.00 - steps 1290 - time: 3.14\n",
      "Episode: 10 - Score: -18.00 - steps 1440 - time: 3.42\n",
      "Episode: 11 - Score: -21.00 - steps 1098 - time: 2.35\n",
      "Episode: 12 - Score: -19.00 - steps 1332 - time: 2.35\n",
      "Episode: 13 - Score: -21.00 - steps 1097 - time: 1.91\n",
      "Episode: 14 - Score: -20.00 - steps 1126 - time: 1.97\n",
      "Episode: 15 - Score: -20.00 - steps 1233 - time: 2.15\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "    total_episodes = 10000\n",
    "\n",
    "    state_size = (80, 80)\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size)\n",
    "    steps = 0\n",
    "    agent.load('pong.h5')\n",
    "    while episode < total_episodes:\n",
    "        # env.render()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        cur_x = preprocess(state)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(state_size)\n",
    "        \n",
    "        prev_x = cur_x\n",
    "        #print(cur_x.shape)\n",
    "\n",
    "        action, prob = agent.act(x)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        #print(reward)\n",
    "        score += reward\n",
    "        \n",
    "        agent.memorize(x, action, prob, reward)\n",
    "        steps += 1\n",
    "        if done:\n",
    "            \n",
    "            episode += 1\n",
    "            agent.train()\n",
    "            print('Episode: %d - Score: %.2f - steps %d - time: %.2f' % (episode, score, steps, time.time()-start_time))\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            prev_x = None\n",
    "            steps = 0\n",
    "            if episode > 1 and episode % 10 == 0:\n",
    "                agent.save('pong.h5')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step (Optional Assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the code to implement algorithms Actor-Critic and A2C (Advantage Actor-Critic). A good reference can be found at https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "- Compare the performance of these three algorithms to which one converges better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-bdb3352f611a>:1: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

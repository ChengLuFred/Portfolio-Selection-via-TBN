{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Market Environment Inplementation\n",
    "The main function of a RL environment is to provide the traning data, 4-elements tuple (s, a, r, s'), via interacting with agents.\n",
    "\n",
    "A series of these tuples consist of training data for RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/cheng/Google Drive/PhD/Research/Portfolio Selection via TBN/codes/')\n",
    "from module.backtesting import *\n",
    "from module.environment_new import *\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class market_environment(vectorized_backtesting):\n",
    "    def __init__ (self, year_range):\n",
    "        super().__init__()\n",
    "        stock_num = self.stock_price.shape[1]\n",
    "        self.action_space = gym.spaces.Box(low = 0.0, high = 1.0, shape=(1, ), dtype=np.float32)\n",
    "        self.action_space_discrete = gym.spaces.Discrete(100)\n",
    "        self.observation_space = gym.spaces.Box(low = -100, high = 100, shape=(1, stock_num + 1), dtype=np.float32)\n",
    "        self.seed()\n",
    "        #self.reset()\n",
    "        self.done = False\n",
    "        self.action = None\n",
    "        self.reward_type = 'sharpe_ratio'\n",
    "        self.year_range = year_range\n",
    "        self.year = year_range[0]\n",
    "    \n",
    "    def seed(self):\n",
    "        pass\n",
    "    def reset(self):\n",
    "        #self.year_range = year_range\n",
    "        self.year = self.year_range[0]\n",
    "        self.done = False\n",
    "        self.action = None\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        '''\n",
    "        Core function in environment. Take action as input, and respond to agent.\n",
    "        Args:\n",
    "            action: np.array\n",
    "                    shrinkage intensity\n",
    "        Returns:\n",
    "            state: np.array\n",
    "            reward: float\n",
    "            done: Bool\n",
    "        '''\n",
    "        if self.done:\n",
    "            # should never reach this point\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.action = action\n",
    "            self.state = self.get_state(action)\n",
    "            self.reward = self.get_reward()\n",
    "            self.done = self.is_done()\n",
    "            self.year += 1\n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)\n",
    "        return [self.state, self.reward, self.done]\n",
    "\n",
    "    def get_state(self, action):\n",
    "        '''\n",
    "        Take agent's action and get back env's next state\n",
    "        Args:\n",
    "            action: a number (shrinkage intensity)\n",
    "        Return:\n",
    "            state - according to state mapping\n",
    "        '''\n",
    "        if not self.done:\n",
    "            #self.action = action\n",
    "            portfolio_mean_return = self.get_portfolio_mean_return(self.year, self.year)\n",
    "            # portfolio_SR = self.get_sharpe_ratio()\n",
    "            # portfolio_TO = self.get_turn_over_for_each_period()\n",
    "            stocks_returns = self.get_stock_mean_returns(self.year)\n",
    "            state = np.append(stocks_returns, portfolio_mean_return).reshape(1,-1)\n",
    "            return state\n",
    "        else:\n",
    "            print('The end of period\\n')\n",
    "            # exit()\n",
    "    \n",
    "    def get_portfolio(self, year):\n",
    "        covariance_shrunk = self.get_shrank_cov(covariance_matrix=self.covariance_aggregate.loc[year - 1].values,\\\n",
    "                                                shrink_target=np.identity(23),\\\n",
    "                                                a=self.action)\n",
    "        portfolio = self.get_GMVP(covariance_matrix = covariance_shrunk)\n",
    "        return portfolio\n",
    "    \n",
    "    def is_done(self):\n",
    "        '''\n",
    "        Check whether agent arrive to the endpoint of the epoc\n",
    "        '''\n",
    "        if self.year != self.year_range[-1]:\n",
    "            self.done = False\n",
    "        else:\n",
    "            self.done = True\n",
    "            \n",
    "        return self.done\n",
    "    \n",
    "    def get_reward(self):\n",
    "        '''\n",
    "        map the reward_type to the reward function\n",
    "        '''\n",
    "        options = {#'excess_return' : self.excess_return,\n",
    "                   #'log_return' : self.log_return,\n",
    "                   #'moving_average' : self.moving_average,\n",
    "                   'sharpe_ratio' : self.get_sharpe_ratio\n",
    "                  }\n",
    "        \n",
    "        reward = options[self.reward_type]()# whether self?\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = np.arange(2000, 2003)\n",
    "env = market_environment(year_range)\n",
    "#env.year = 2000\n",
    "#env.action = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.77891805,  0.27538629,  0.09018561,  0.55473796,  0.09526784,\n",
       "         -0.11049116,  0.02016256,  0.10128134, -0.22888538, -0.12259082,\n",
       "          0.17466226,  0.13547645, -0.0910565 ,  0.27295561,  0.40582724,\n",
       "         -0.82923405,  0.27705591, -0.19094159,  0.56494237,  0.92530383,\n",
       "         -0.11271183,  0.44684607, -0.13042818,  0.07586234]]),\n",
       " 0.09188408504593096,\n",
       " False]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.array([1])\n",
    "env.step(action)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f99ea47d1967c482d1a96e8f57146526516fa4859dddae9b508497402c9084"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('portfolio': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
